---
refID: '0009'
title: Minimiser la taille des réponses
createdAt: 2025-12-02T12:00:00.000Z
updatedAt: 2025-12-02T12:00:00.000Z
language: fr
published: true
refType: RIA
versions:
  - version: 1.0.0
    idRef: '0009'
people: TBD
responsible:
  - responsible: src/content/personas/fr/poamoa.mdx
lifecycle: 1-specification
rgesn: '6.1'
environmental_impact: 3
priority_implementation: 3
moe: 2
tiers: user-device
saved_resources:
  - cpu
  - network
validations:
  - rule: >-
      Le nombre de requêtes sans limitation de la taille de la réponse associée est de
    maxValue: 0
---

## Description

La taille de la réponse générée par le LLM a une grande influence sur la quantité de ressources utilisées, comme le
nombre la latence associée (qui est un proxy pour le temps de calcul) ou des études comme [celle de Adamska et al.](https://arxiv.org/pdf/2503.10666).

Limiter la taille des réponses générées est donc un levier important pour réduire les ressources utilisées et
les impacts environnementaux associés.

## Exemple

Il est possible de limiter la taille des réponses dans le prompt avec une instruction du type « répondre en moins de 20 lignes ».
Les outils d’intégration de LLM permettent également de conjuguer ce paramètre programatiquement, par exemple avec LangChain4J :

```
MistralAiChatModel model = MistralAiChatModel.builder()
    .apiKey(ApiKeys.MISTRALAI_API_KEY)
    .modelName(MistralAiChatModelName.MISTRAL_MEDIUM_LATEST)
    .maxTokens(250)
    .build();
```
