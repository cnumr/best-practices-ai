---
refID: '0021'
title: Compresser les modèles
createdAt: 2025-12-02T12:00:00.000Z
updatedAt: 2025-12-02T12:00:00.000Z
language: fr
published: true
refType: RIA
versions:
  - version: 1.0.0
    idRef: '0021'
people: TBD
responsible:
  - responsible: src/content/personas/fr/codeuseur-developpeuser.mdx
lifecycle: 3-developement
rgesn: 9.6
environmental_impact: 3
priority_implementation: 3
moe: 4
tiers: datacenter
saved_resources:
  - cpu
  - ram
  - storage
validations:
  - rule: >-
      Le nombre de modèles utilisés sans évaluation de la pertinence d'une compression est de
    maxValue: 0
---

## Description

Il existe des méthodes pour réduire la taille des modèles après leur entrainement, en préservant leur performance ou
en limitant la perte :
* Knowledge distillation : Transfert des connaissances d’un modèle d’IA “professeur” déjà formé vers un modèle “élève” plus petit.
* Pruning : Méthode de compression qui consiste à supprimer les connexions ayant le moins de poids dans un réseau (jusqu’à 90%) ainsi que les éléments et liens redondants.
* Quantification : Réduction de la précision des données, en réduisant le nombre de bits utiles à la réalisation des opérations ou au décryptage des données.
